<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Estadística on Daniel Alcaide</title>
    <link>https://danielalcaide.com/tags/estad%C3%ADstica/</link>
    <description>Recent content in Estadística on Daniel Alcaide</description>
    <generator>Hugo -- 0.151.0</generator>
    <language>es</language>
    <lastBuildDate>Tue, 14 Dec 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://danielalcaide.com/tags/estad%C3%ADstica/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Estandarización de datos</title>
      <link>https://danielalcaide.com/estandarizacion_de_datos/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://danielalcaide.com/estandarizacion_de_datos/</guid>
      <description>&lt;p&gt;La &lt;strong&gt;estandarización&lt;/strong&gt; de los datos es un paso común en el preprocesamiento. Ésta se puede definir como la acción de cambiar los datos para que estén centrados en el 0 y tengan una desviación estándar de 1. El objetivo es llevar las variables con diferentes unidades a una común. Muchas tareas de aprendizaje automático son sensibles a las magnitudes de los datos y se supone que la estandarización elimina esos factores. Por ejemplo, el método de &lt;em&gt;k-nearest neighbors&lt;/em&gt; es sensible a las magnitudes de las variables, por lo que se deben estandarizar los datos. En cambio, los métodos basados en árboles (&lt;em&gt;tree-based methods&lt;/em&gt;) no son sensibles a los diferentes rangos de las variables, por lo que la estandarización no es necesaria.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
